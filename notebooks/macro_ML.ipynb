{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading utils/config.json\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline \n",
    "\n",
    "import sys, os\n",
    "from utils.basic_utils import *\n",
    "from utils.pricing import *\n",
    "from utils.fundamental import *\n",
    "from utils import ml_utils as mu\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\n",
    "from sklearn.ensemble import forest\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, log_loss, precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, roc_auc_score\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# environment variables\n",
    "freq = '1d'\n",
    "cuts = { '1d': [-1, -0.1, -.02, .02, .1, 1.] }\n",
    "cut_range = cuts[freq]\n",
    "fwd_ret_labels = [\"bear\", \"short\", \"neutral\", \"long\", \"bull\"]\n",
    "\n",
    "# pricing, refresh once\n",
    "benchSL, sectorSL, riskSL, rateSL, bondSL, commSL, currSL = \\\n",
    "    config['benchmarks'], config['sectors'], config['risk'], config['rates'], \\\n",
    "    config['bonds'], config['commodities'], config['currencies']\n",
    "symbols_list = benchSL + sectorSL + riskSL + rateSL + bondSL + commSL + currSL\n",
    "\n",
    "keep_bench = excl(benchSL, ['^STOXX50E', '^AXJO'])\n",
    "keep_fx = excl(currSL, ['HKD=X', 'MXN=X', 'AUDUSD=X', 'NZDUSD=X', 'TWD=X', 'CLP=X', 'KRW=X'])\n",
    "keep_sect = excl(sectorSL, ['SPY', 'QQQ', 'DIA', 'IWM', 'XLC', 'XLRE'])\n",
    "keep_bonds = ['LQD', 'HYG']\n",
    "\n",
    "include = riskSL + keep_bench + keep_sect + rateSL + keep_fx + keep_bonds\n",
    "invert = ['EURUSD=X', 'GBPUSD=X']\n",
    "incl_price = riskSL\n",
    "\n",
    "bench = '^GSPC'\n",
    "y_col = 'fwdReturn'\n",
    "pred_fwd_windows = [20, 60, 120]\n",
    "rate_windows = [20, 60]\n",
    "sec_windows, stds = [5, 20, 60], 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     1,
     37,
     78,
     185
    ]
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def create_ds(px_close, context):\n",
    "    print('create_ds')\n",
    "    train_model = context['train_model']    \n",
    "    portion = context['portion']\n",
    "    verbose = context['verbose']\n",
    "    # average the return of the next periods\n",
    "    # select only rows where Y variable is not null\n",
    "    ds_idx = px_close.dropna(subset=[bench]).index    \n",
    "\n",
    "    df_large = pd.DataFrame()\n",
    "    rate_ft_df = rate_feats(px_close[rateSL], rate_windows) # rate transforms\n",
    "    df_large[rate_ft_df.columns] = rate_ft_df\n",
    "\n",
    "    # price momentum transforms\n",
    "    super_list = []\n",
    "    for ticker in include:\n",
    "        inv = ticker in invert\n",
    "        incl_px = ticker in incl_price\n",
    "        df = px_close[ticker]\n",
    "        ft_df = px_mom_feats(df, ticker, stds, inv, incl_px, sec_windows)\n",
    "        super_list.append(ft_df.drop_duplicates())\n",
    "    df_large = pd.concat(super_list, axis=1).sort_index()\n",
    "    df_large = df_large.loc[ds_idx, :] # drop NAs before discretizing\n",
    "    \n",
    "    if train_model:\n",
    "        Y = px_fwd_rets(px_close.loc[ds_idx, bench], bench, pred_fwd_windows).mean(axis=1)\n",
    "        df_large[y_col] = Y\n",
    "        # reduce dataset?\n",
    "        if portion < 100e-2: _, df_large = train_test_split(\n",
    "            df_large, test_size=portion, random_state=42)\n",
    "        if verbose:\n",
    "            print('create_ds >> df_large.shape: ', df_large.shape)\n",
    "            print('Y.shape: ', Y.shape)\n",
    "    \n",
    "    return ds_idx, df_large\n",
    "\n",
    "def pre_process_ds(raw_df, context):\n",
    "    print('pre_process_ds')\n",
    "    train_model = context['train_model']\n",
    "    pred_batch = context['predict_batch']    \n",
    "    fill_on, imputer_on, scaler_on = context['fill'], context['impute'], context['scale']\n",
    "    (path, train_cols) = context['trained_cols']\n",
    "    test_sz, verbose = context['test_size'], context['verbose']\n",
    "\n",
    "    scaler = StandardScaler()    \n",
    "    imputer = SimpleImputer(\n",
    "        missing_values=np.nan, \n",
    "        strategy='median', copy=False)\n",
    "    X_cols = excl(raw_df.columns, [y_col])\n",
    "\n",
    "    raw_df.replace([np.inf, -np.inf], np.nan, inplace=True)    \n",
    "    if scaler_on: raw_df[X_cols] = scaler.fit_transform(raw_df[X_cols])\n",
    "    \n",
    "    pred_X = X_train = X_test = y_train = y_test = None\n",
    "    if train_model:\n",
    "        if fill_on: raw_df.loc[:, X_cols].fillna(method=fill_on, inplace=True)\n",
    "\n",
    "        # discretize forward returns into classes\n",
    "        raw_df.dropna(subset=[y_col], inplace=True)\n",
    "        raw_df.loc[:, y_col] = discret_rets(raw_df[y_col], cut_range, fwd_ret_labels)\n",
    "        raw_df.dropna(subset=[y_col], inplace=True) # no nas in y_col\n",
    "        print(sample_wgts(raw_df[y_col]))\n",
    "        raw_df.loc[:, y_col] = raw_df[y_col].astype(str) # class as string        \n",
    "        \n",
    "        if imputer_on: raw_df.loc[:, X_cols] = imputer.fit_transform(raw_df[X_cols])\n",
    "        else: raw_df.dropna(inplace=True)\n",
    "\n",
    "        X, y = raw_df.drop(columns=y_col), raw_df[y_col]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_sz, random_state=42)\n",
    "        np.save(path + train_cols, X_train.columns) # save feature order        \n",
    "    else:\n",
    "        pred_X = raw_df.iloc[-pred_batch:,:].dropna(axis=0)\n",
    "    \n",
    "    [print(x.shape) for x in (pred_X, X_train, X_test, y_train, y_test) if x is not None]\n",
    "    return pred_X, X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_ds(context):\n",
    "\n",
    "    print('Benchmark: {}, Y: {}, Include: {}, invert: {}, include price: {}'.format(\n",
    "    bench, y_col, include, invert, incl_price))\n",
    "\n",
    "    context['train_model'] = True\n",
    "    ml_path = context['ml_path']\n",
    "    grid_search = context['grid_search']\n",
    "    verbose = context['verbose']\n",
    "\n",
    "    # create and pre-process datasets\n",
    "    _, df_large = create_ds(px_close, context)\n",
    "    pred_X, X_train, X_test, y_train, y_test = pre_process_ds(df_large, context)\n",
    "\n",
    "    # RandomForestClassifier\n",
    "    features = X_train.shape[1]\n",
    "    best_params = { # best from GridSearch\n",
    "        'n_estimators': 50, \n",
    "        'max_features': features // 4, \n",
    "        'max_depth': 30,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'random_state': 2,\n",
    "        'n_jobs': -1}\n",
    "    if grid_search:\n",
    "        print('GridSearchCV for RandomForestClassifier')\n",
    "        param_grid = {\n",
    "            'n_estimators': [50],\n",
    "            'max_features': [features // x for x in range(1,10,1)],\n",
    "            'max_depth': 30,\n",
    "            'min_samples_split': 2,\n",
    "            'min_samples_leaf': 1,            \n",
    "            'random_state': np.arange(0, 5, 1),\n",
    "        }\n",
    "        clf = GridSearchCV(RandomForestClassifier(random_state=42),\n",
    "                           param_grid, n_jobs=-1,\n",
    "                           cv=5, iid=True, verbose=verbose)\n",
    "        clf.fit(X_train, y_train)\n",
    "        if verbose: print_cv_results(\n",
    "            clf, X_train, X_test, y_train, y_test,\n",
    "            feat_imp=True, top=20)\n",
    "        best_params = clf.best_params_\n",
    "    clf1 = RandomForestClassifier(**best_params)\n",
    "    clf1.fit(X_train, y_train)\n",
    "    print('RandomForestClassifier scores: Train {}, Test {}'.format(\n",
    "    clf1.score(X_train, y_train), clf1.score(X_test, y_test)))\n",
    "\n",
    "    # MLPClassifier\n",
    "    best_params = {\n",
    "        'solver': 'sgd',\n",
    "        'max_iter': 2000,    \n",
    "        'activation': 'relu', \n",
    "        'alpha': 0.01, \n",
    "        'hidden_layer_sizes': (features // 4),\n",
    "        'learning_rate': 'adaptive', \n",
    "        'random_state': 0, }\n",
    "    if grid_search:\n",
    "        print('GridSearchCV for MLPClassifier')\n",
    "        param_grid = {\n",
    "            'solver': ['sgd'], # ['lbfgs', 'sgd', 'adam']\n",
    "            'max_iter': [2000], # [400, 1000, 2000]\n",
    "            'activation': ['relu'], # ['logistic', 'tanh', 'relu']\n",
    "            'alpha': [10.0 ** -2], # 10.0 ** -np.arange(2, 5, 1)\n",
    "            'learning_rate' : ['adaptive'], # ['constant', 'adaptive']\n",
    "            'hidden_layer_sizes': [features // 4], #[features // x for x in range(2,10,2)]\n",
    "            'random_state': np.arange(0, 5, 1)\n",
    "        }\n",
    "        clf = GridSearchCV(MLPClassifier(random_state=42), param_grid, n_jobs=-1, cv=5,\n",
    "                          iid=True, verbose=verbose)\n",
    "        clf.fit(X_train, y_train)\n",
    "        if verbose: mu.print_cv_results(\n",
    "            clf, (X_train, X_test, y_train, y_test),\n",
    "            feat_imp=False, top=20)\n",
    "        best_params = clf.best_params_\n",
    "\n",
    "    clf2 = MLPClassifier(**best_params)\n",
    "    clf2.fit(X_train, y_train)\n",
    "    print('MLPClassifier scores Train {}, Test {}'.format(\n",
    "    clf2.score(X_train, y_train), clf2.score(X_test, y_test)))\n",
    "\n",
    "    best_params = { # best from GridSearch\n",
    "        'n_estimators': 50, \n",
    "        'max_features': features // 2, \n",
    "        'max_depth': 20,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1,\n",
    "        'random_state': 2,\n",
    "        'n_jobs': -1}\n",
    "    # ExtraTreesClassifier\n",
    "    clf3 = ExtraTreesClassifier(**best_params)\n",
    "    clf3.fit(X_train, y_train)\n",
    "    print('ExtraTreesClassifier scores Train {}, Test {}'.format(\n",
    "    clf3.score(X_train, y_train), clf3.score(X_test, y_test)))\n",
    "\n",
    "    for vote in ['hard', 'soft']:\n",
    "        eclf = VotingClassifier(\n",
    "            estimators=[('rf', clf1), ('mlp', clf2), ('et', clf3)],\n",
    "            voting=vote)\n",
    "        clf = eclf.fit(X_train, y_train)\n",
    "        print('VotingClassifier scores Train {}, Test {}'.format(\n",
    "                clf.score(X_train, y_train), clf.score(X_test, y_test)))\n",
    "\n",
    "        os.makedirs(ml_path, exist_ok=True)\n",
    "        fname = ml_path + f'macro_ML_{vote}.pkl'\n",
    "        joblib.dump(clf, fname)\n",
    "        print('Saved ', fname)\n",
    "        \n",
    "def predict_ds(context):\n",
    "    context['train_model'] = False    \n",
    "    ml_path = context['ml_path']\n",
    "    verbose = context['verbose']\n",
    "    (path, train_cols) = context['trained_cols']    \n",
    "    \n",
    "    ds_idx, df_large = create_ds(px_close, context)\n",
    "    pred_X, _, _, _, _ = pre_process_ds(df_large, context)    \n",
    "    print('pred_X.shape', pred_X.shape)\n",
    "    \n",
    "    # ensure prediction dataset is consistent with trained model\n",
    "    trained_cols = np.load(path + train_cols) # save feature order    \n",
    "    missing_cols = [x for x in trained_cols if x not in pred_X.columns]\n",
    "    pred_X = pd.concat([pred_X, pd.DataFrame(columns=missing_cols)], axis=1)\n",
    "    pred_X[missing_cols] = 0\n",
    "    pred_X = pred_X[list(trained_cols)]    \n",
    "\n",
    "    bench_df = px_close.loc[pred_X.index, bench].to_frame()\n",
    "    for vote in ['hard', 'soft']:\n",
    "        fname = ml_path + f'macro_ML_{vote}.pkl'\n",
    "        clf = joblib.load(fname) # load latest models\n",
    "        print('Loaded', fname)\n",
    "        preds = clf.predict(pred_X)\n",
    "        pred_class = np.array([fwd_ret_labels.index(x) for x in preds])        \n",
    "        bench_df[f'{vote}_pred_class'] = pred_class\n",
    "        bench_df[f'{vote}_pred_label'] = preds\n",
    "        if vote == 'soft':\n",
    "            probs = clf.predict_proba(pred_X)\n",
    "            pred_prob = np.argmax(probs, axis=1)\n",
    "            bench_df[f'{vote}_confidence'] = [x[np.argmax(x)] for x in probs] # higest prob\n",
    "            prob_df = pd.DataFrame(probs, index=bench_df.index, columns=clf.classes_)\n",
    "            bench_df = pd.concat([bench_df, prob_df[fwd_ret_labels]], axis=1)\n",
    "        bench_df.dropna(subset=[bench], inplace=True)\n",
    "\n",
    "    # store in S3\n",
    "    s3_path = context['s3_path']\n",
    "    s3_df = pred_df.reset_index(drop=False)\n",
    "    rename_col(s3_df, 'index', 'pred_date')\n",
    "    csv_store(s3_df, s3_path, csv_ext.format(tgt_date[0]))\n",
    "        \n",
    "    return bench_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the date range of key series currently working only with:\n",
    "- Benchmarks: DOW, Nasdaq, SPY, and Russell 2000\n",
    "- Sectors ETFs (excluding benchmarks)\n",
    "- Rates: 2, 5, 10, and 30 year treasuries\n",
    "- Risk: VIX\n",
    "- Hard currencies (JPY, EUR, GBP and Dollar Index DXY)\n",
    "- Corporate Bonds: Investment Grade (LQD), High Yield (JNK and HYG)\n",
    "\n",
    "Ideally would like to go back further are include:\n",
    "- Commodities price change\n",
    "- Yields Spreads from Corporate Spreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PRE-PROCESS DATASET\n",
    "# train_idx, df_large = create_ds(px_close)\n",
    "# pred_X, X_train, X_test, y_train, y_test = pre_process_ds(df_large, context)\n",
    "# pred_X.shape\n",
    "# check for NAs, there should be none\n",
    "# df = X_train\n",
    "# df.loc[:,df.isna().any(0).values].columns\n",
    "# df.tail().isna().any(0).sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = {\n",
    "    'portion': 100e-2,\n",
    "    'trained_cols': ('../ML/', 'macro_train_cols.npy'),\n",
    "    'fill': 'bfill',\n",
    "    'impute': True,\n",
    "    'scale': True,\n",
    "    'test_size': .20,\n",
    "    'predict_batch': 252,\n",
    "    'ml_path': '../ML/',\n",
    "    'grid_search': False,\n",
    "    's3_path': 'recommend/micro_ML/',\n",
    "    'verbose': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-04-03'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(today_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px_close = get_mults_pricing(include, freq, verbose=context['verbose']);\n",
    "px_close.drop_duplicates(inplace=True)\n",
    "print('px_close.shape', px_close.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "context['train_model'] = True\n",
    "%time train_ds(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time pred_df = predict_ds(context)\n",
    "pred_df.tail(5).round(3).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store / Read S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(path, fname) = context['s3_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store in S3\n",
    "s3_df = pred_df.reset_index(drop=False)\n",
    "rename_col(s3_df, 'index', 'pred_date')\n",
    "csv_store(s3_df, s3_path, csv_ext.format(storeDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from S3\n",
    "pred_df = pd.read_csv(\n",
    "    csv_load(f'{s3_path}{storeDate}'), \n",
    "    index_col='pred_date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_class_cols = filter_cols(pred_df.columns, \"pred_class\")\n",
    "pred_df.loc[:,[bench] + pre_class_cols].plot(\n",
    "    secondary_y=pre_class_cols, figsize=(15, 5));\n",
    "pred_df[fwd_ret_labels].plot.area(\n",
    "        title='Prediction Probabilities',\n",
    "        figsize=(15, 2), ylim=(0, 1), cmap='RdYlGn');\n",
    "f'Confidence Mean: {pred_df[\"soft_confidence\"].mean().round(3)}, Median {pred_df[\"soft_confidence\"].median().round(3)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# average the return of the next periods\n",
    "# select only rows where Y variable is not null\n",
    "train_idx = px_close.dropna(subset=[bench]).index\n",
    "Y = px_fwd_rets(px_close.loc[train_idx, bench], bench, pred_fwd_windows).mean(axis=1)\n",
    "Y.plot.hist(bins=50, title='Distribution of Forwards Returns')\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_large = pd.DataFrame()\n",
    "\n",
    "# rate transforms\n",
    "rate_ft_df = rate_feats(px_close[rateSL], rate_windows)\n",
    "df_large[rate_ft_df.columns] = rate_ft_df\n",
    "\n",
    "# price momentum transforms\n",
    "super_list = []\n",
    "for ticker in include:\n",
    "    inv = ticker in invert\n",
    "    incl_px = True if ticker in incl_price else False\n",
    "    df = px_close[ticker]\n",
    "    ft_df = px_mom_feats(df, ticker, stds, inv, incl_px, sec_windows)\n",
    "    super_list.append(ft_df.drop_duplicates())\n",
    "df_large = pd.concat(super_list, axis=1).sort_index()\n",
    "\n",
    "df_large[y_col] = Y\n",
    "\n",
    "# drop NAs before discretizing\n",
    "df_large = df_large.loc[train_idx, :]\n",
    "\n",
    "print(df_large.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)\n",
    "df_large.to_parquet('tmp/macro-ds-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_large = pd.read_parquet('tmp/macro-ds-large')\n",
    "df_large.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Trims dataset in case it's too large for experimentation\n",
    "# Reduce dataset for experimentation\n",
    "# Note that experiment dataset is not stratified\n",
    "_, df_raw = train_test_split(df_large, test_size=100e-2, shuffle=False, )\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# keeps a chunk of the pre-processed dataset for predictions later\n",
    "# take everything but Y var\n",
    "look_back = 252*3\n",
    "recent_X = df_large.iloc[-look_back:,:-1].copy()\n",
    "print(recent_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# discretize returns into classes\n",
    "df_raw.dropna(subset=[y_col], inplace=True)\n",
    "df_raw.loc[:, y_col] = discret_rets(df_raw[y_col], cut_range, fwd_ret_labels)\n",
    "\n",
    "y_col_dist = sample_wgts(df_raw[y_col], fwd_ret_labels)\n",
    "(y_col_dist[fwd_ret_labels]).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# how many empty values?\n",
    "# (df_raw.isna().sum() / df_raw.count()).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_raw.fillna(method='ffill', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Mean based imputer\n",
    "imputer_on, scaler_on = True, True\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median', copy=False)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df_raw.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "pre_ml_df = df_raw.copy()\n",
    "pre_ml_df.dropna(subset=[y_col], inplace=True)\n",
    "pre_ml_df.loc[:, y_col] = pre_ml_df[y_col].astype(str)\n",
    "\n",
    "X_cols = excl(pre_ml_df.columns, [y_col])\n",
    "\n",
    "if imputer_on: pre_ml_df.loc[:, X_cols] = imputer.fit_transform(pre_ml_df[X_cols])\n",
    "else: pre_ml_df.dropna(inplace=True)\n",
    "if scaler_on: pre_ml_df.loc[:, X_cols] = scaler.fit_transform(pre_ml_df[X_cols])\n",
    "\n",
    "X, y = pre_ml_df.drop(columns=y_col), pre_ml_df[y_col]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.20, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "# best params from GridSearch\n",
    "rfc_params = {\n",
    "    'max_features': 40, 'n_estimators': 100, 'random_state': 7}\n",
    "\n",
    "clf1 = RandomForestClassifier(**rfc_params, warm_start=True)\n",
    "\n",
    "clf1.fit(X_train, y_train)\n",
    "scores = clf1.score(X_train, y_train), clf1.score(X_test, y_test)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# MLPClassifier\n",
    "# best params from GridSearch\n",
    "mlp_params = {\n",
    "    'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': 65, \n",
    "    'learning_rate': 'adaptive', 'max_iter': 200, \n",
    "    'random_state': 3, 'solver': 'lbfgs'}\n",
    "\n",
    "clf2 = MLPClassifier(**mlp_params, warm_start=True)\n",
    "\n",
    "clf2.fit(X_train, y_train)\n",
    "scores = clf2.score(X_train, y_train), clf2.score(X_test, y_test)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ExtraTreesClassifier\n",
    "clf3 = ExtraTreesClassifier(\n",
    "    n_estimators=100, max_depth=None, \n",
    "    min_samples_split=2, random_state=0, warm_start=True)\n",
    "\n",
    "clf3.fit(X_train, y_train)\n",
    "scores = clf3.score(X_train, y_train), clf3.score(X_test, y_test)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('rf', clf1), ('mlp', clf2), ('et', clf3)], \n",
    "    voting=vote)\n",
    "\n",
    "clf = eclf.fit(X_train, y_train)\n",
    "scores = clf.score(X_train, y_train), clf.score(X_test, y_test)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(ml_path, exist_ok=True)\n",
    "fname = ml_path + f'macro_ML_{vote}.pkl'\n",
    "joblib.dump(clf, fname)\n",
    "f'Saved {fname}'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load(fname)\n",
    "print('Loaded', fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# predict historically including recent dates\n",
    "if imputer_on: recent_X.loc[:, X_cols] = imputer.fit_transform(recent_X[X_cols])\n",
    "else: recent_X.dropna(inplace=True)\n",
    "if scaler_on: recent_X.loc[:, X_cols] = scaler.fit_transform(recent_X[X_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert recent_X.shape[1] == X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assert recent_X.shape[1] == X_train.shape[1]\n",
    "preds = clf.predict(recent_X)\n",
    "assert recent_X.shape[0] == preds.shape[0]\n",
    "\n",
    "pred_class = np.array([fwd_ret_labels.index(x) for x in preds])\n",
    "\n",
    "SPY = px_close.loc[recent_X.index, bench].to_frame()\n",
    "SPY['pred_class'] = pred_class\n",
    "SPY['pred_label'] = preds\n",
    "\n",
    "if vote == 'soft':\n",
    "    probs = clf.predict_proba(recent_X)\n",
    "    pred_prob = np.argmax(probs, axis=1)\n",
    "    SPY.loc[:, 'high_prob'] = [x[np.argmax(x)] for x in probs] # higest prob\n",
    "    SPY.loc[:, 'pred_prob'] = probs[range(pred_class.shape[0]), pred_class] # predict class prob\n",
    "\n",
    "SPY.dropna(subset=[bench], inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# why are predict classes and prob distribution do not math?\n",
    "# preds, pred_class, probs[-10:]\n",
    "# df = pd.DataFrame()\n",
    "# df['pred_class'] = pred_class\n",
    "# df['pred_prob'] = probs[range(pred_class.shape[0]), pred_class]\n",
    "# df.plot(secondary_y='pred_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot predictions\n",
    "SPY['pred_class'] = pred_class\n",
    "SPY['pred_label'] = preds\n",
    "SPY.loc[:,[bench, 'pred_class']].plot(secondary_y='pred_class', figsize=(15, 5));\n",
    "\n",
    "if vote == 'soft':\n",
    "    SPY[['high_prob']].plot(\n",
    "        title='Historical predictions',\n",
    "        figsize=(15, 2), ylim=(0, 1),);\n",
    "    SPY[['pred_prob']].plot(\n",
    "        title='Probability of highest & predicted class',\n",
    "        figsize=(15, 2), ylim=(0, 1),);\n",
    "    \n",
    "    prob_df = pd.DataFrame(probs, columns=fwd_ret_labels, index=recent_X.index)\n",
    "    prob_df.plot(\n",
    "        title='Class probabilities',\n",
    "        figsize=(15, 8), subplots=True, ylim=(0,1));    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if vote == 'soft':\n",
    "    SPY[['high_prob', 'pred_prob']].plot.hist(\n",
    "        bins=50, xlim=(0,1), title='Confidence Distribution')\n",
    "    print('Probability for predicted class: Mean %.f, median %.f, ' % (\n",
    "        SPY.pred_prob.mean() * 100,\n",
    "        SPY.pred_prob.median() * 100))\n",
    "    print('Probability for highest class: Mean %.f, median %.f, ' % (\n",
    "        SPY.high_prob.mean() * 100,\n",
    "        SPY.high_prob.median() * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "SPY.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "draw_tree(clf1.estimators_[0], X, precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_fi(clf1, X, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, clf1.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Confussion Matrix\\n', confusion_matrix(clf.predict(X_test), y_test, labels=fwd_ret_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Classificaton report\\n', classification_report(clf.predict(X_test), y_test, target_names=fwd_ret_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fi = rf_feat_importance(clf1, pd.DataFrame(X)); fi[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "clf1.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "[plt.plot(x.tolist(), alpha=0.1, color='b', marker='*', linewidth=0) for x in clf1.predict_proba(X_test)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Gridsearches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# GridSearchCV for RandomForestClassifier\n",
    "verbose = True\n",
    "parameters = {\n",
    "    'n_estimators': [100],\n",
    "    'max_features': ['sqrt'],\n",
    "    'random_state': np.arange(0, 5, 1),}\n",
    "clf = GridSearchCV(RandomForestClassifier(), \n",
    "                   parameters, n_jobs=-1, \n",
    "                   cv=5, iid=True, verbose=5)\n",
    "clf.fit(X_train, y_train)\n",
    "if verbose: print_cv_results(\n",
    "    clf, X_train, X_test, y_train, y_test, \n",
    "    feat_imp=True, top=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# GridSearchCV for MLPClassifier\n",
    "parameters = {\n",
    "    'solver': ['lbfgs'], # ['lbfgs', 'sgd', 'adam']\n",
    "    'max_iter': [200], # [200, 400, 600]\n",
    "    'activation': ['relu'], # ['logistic', 'tanh', 'relu']\n",
    "    'alpha': 10.0 ** -np.arange(2, 5, 1), # 10.0 ** -np.arange(2, 5, 1)\n",
    "    'learning_rate' : ['adaptive'], # ['constant', 'adaptive']\n",
    "    'hidden_layer_sizes': np.arange(5, X_train.shape[1] // 3, int(X_train.shape[1] * 0.1)), # np.arange(5, 50, 10)\n",
    "    'random_state': np.arange(0, 5, 1)} # np.arange(0, 10, 2)\n",
    "clf = GridSearchCV(MLPClassifier(), parameters, n_jobs=-1, cv=5,\n",
    "                  iid=True, verbose=5)\n",
    "clf.fit(X_train, y_train)\n",
    "if verbose: print_grid_search(\n",
    "    clf, X_train, y_train, X_test, y_test, \n",
    "    feat_imp=False, top=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Feature construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# FX\n",
    "FX = ['EURUSD=X', 'JPY=X', 'GBPUSD=X', 'CNY=X', 'CHF=X', 'DX-Y.NYB']\n",
    "invert = ['EURUSD=X', 'GBPUSD=X']\n",
    "df_raw = px_close[FX]\n",
    "df_raw.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fx_df = df_raw.copy()\n",
    "fx_df[invert] = (1 / df_raw[invert])\n",
    "fx_df.tail(252)\n",
    "fx_chg_df = fx_df.pct_change().rolling(60).sum()\n",
    "fx_chg_df.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rates = px_close[rateSL]\n",
    "rates.plot.kde(alpha=1, title='Yield Curve Rate Distribution')\n",
    "rate_feats(rates, [60]).plot.kde(title='10Yr - 3M Spread')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Uses px_mom_feats for FX transforms\n",
    "FX = ['EURUSD=X', 'JPY=X', 'GBPUSD=X', 'CNY=X', 'CHF=X', 'DX-Y.NYB']\n",
    "invert = ['EURUSD=X', 'GBPUSD=X']\n",
    "\n",
    "ndf = pd.DataFrame() \n",
    "for fx in FX:\n",
    "    inv = True if fx in invert else False\n",
    "    df = get_symbol_pricing(fx)\n",
    "    ft_df = px_mom_feats(df, fx, 1, inv, True)\n",
    "    ndf[ft_df.columns] = ft_df\n",
    "ndf.plot.hist(alpha=0.5, legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ft_df = px_mom_feats(get_symbol_pricing('^VIX'), '^VIX', 2, False, True, [20])\n",
    "print(ft_df.columns)\n",
    "ft_df.iloc[:,:].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "symbol = '^GSPC'\n",
    "px_df = get_symbol_pricing(symbol)\n",
    "\n",
    "ft_df = px_mom_feats(px_df, symbol)\n",
    "# ft_df.plot(subplots=True, figsize=(10,15));\n",
    "ft_df.plot.hist(alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ft_df = px_fwd_rets(px_df, symbol)\n",
    "ft_df.plot.hist(alpha=0.5);\n",
    "ft_df[symbol] = px_df['close']\n",
    "ft_df.tail(252*4).plot(secondary_y=symbol, figsize=(10,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# average different time frames of fwd rets for all benchmarks\n",
    "# do we try to estimate the average of all benchmarks or just one?\n",
    "# do we try to estimate three type of fwd rets [20, 60, 120] or just one?\n",
    "\n",
    "# calculate forward returns for all bechmarks\n",
    "fwd_rets_df = pd.DataFrame()\n",
    "for s in benchSL:\n",
    "    px_df = get_symbol_pricing(s)\n",
    "    fwd_ret = px_fwd_rets(px_df, s)\n",
    "    fwd_rets_df[fwd_ret.columns] = fwd_ret\n",
    "fwd_rets_df.tail()\n",
    "\n",
    "[fwd_rets_df[[x for x in fwd_rets_df.columns if y in x]].mean(axis=1).plot.hist(alpha=0.5)\n",
    "     for y in ['g20', 'g60', 'g120']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "locs = [-int(x**3) for x in range(1, 7, 1)]\n",
    "px_close[rateSL].iloc[locs, :].T.plot(title='Historical Yield Curve');\n",
    "#f'Yield curve for the last {locs} days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# show correlations with Y variables\n",
    "show = ['mktFwdChg1m', 'mktFwdChg3m', 'mktFwdChg6m']\n",
    "ml_ds_df.corr()[show]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# series distributions\n",
    "%time ml_ds_df.hist(figsize=(15,15));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# scatter plots\n",
    "%time pd.plotting.scatter_matrix(ml_ds_df, alpha=0.1, figsize=(40, 40));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# historical returns for 1, 3, and 6 months\n",
    "secpx['mktHistPctChgStds'] = SPY_pct_chg.apply(sign_compare, args=(SPY_pct_chg.std(),))\n",
    "secpx['mktHistPctChg1m'] = SPY_close.pct_change(periods=20)\n",
    "secpx['mktHistPctChg3m'] = SPY_close.pct_change(periods=60)\n",
    "secpx['mktHistPctChg6m'] = SPY_close.pct_change(periods=120)\n",
    "secpx['mktPerc52WkHigh'] = (SPY_close / SPY_close.rolling(252).max())\n",
    "secpx['mktPerc52WkLow'] = (SPY_close / SPY_close.rolling(252).min())\n",
    "\n",
    "show = ['mktHistPctChgStds', 'mktHistPctChg1m', 'mktHistPctChg3m', \n",
    "        'mktHistPctChg6m']\n",
    "secpx[show].hist();\n",
    "secpx[show].tail(252*4).plot(secondary_y='mktHistPctChgStds');\n",
    "show = ['mktPerc52WkLow']\n",
    "secpx[show].tail(252*4).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Forward returns, 1w, 1m, 3m\n",
    "SPY_close.pct_change(5).shift(-5)\n",
    "secpx['mktFwdChg1m'] = SPY_close.pct_change(20).shift(-20)\n",
    "secpx['mktFwdChg3m'] = SPY_close.pct_change(60).shift(-60)\n",
    "secpx['mktFwdChg6m'] = SPY_close.pct_change(120).shift(-120)\n",
    "\n",
    "show = ['mktFwdChg1m', 'mktFwdChg3m', 'mktFwdChg6m']\n",
    "secpx[show].hist();\n",
    "secpx[show].tail(252*4).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show = [bench_symbol, 'mktFwdChg1m', 'mktFwdChg3m', 'mktFwdChg6m']\n",
    "secpx[show].tail(252*4).plot(secondary_y=bench_symbol, figsize=(12,5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cumulative deltas and gap analysis\n",
    "secpx['pctChgStds'] = pct_chg.apply(sign_compare, args=(pct_chg.std(),))\n",
    "secpx['openGap1m'] = (openpx - closepx.shift(1)).rolling(20).sum()\n",
    "secpx['hlDelta1m'] = (highpx - lowpx).rolling(20).sum()\n",
    "secpx['cumChg1m'] = (closepx - closepx.shift(1)).rolling(20).sum()\n",
    "secpx['cumChg3m'] = (closepx - closepx.shift(1)).rolling(60).sum()\n",
    "secpx['cumChg6m'] = (closepx - closepx.shift(1)).rolling(120).sum()\n",
    "secpx['perc52WkLow'] = (closepx / closepx.rolling(252).min())\n",
    "\n",
    "show = ['pctChgStds', 'openGap1m', 'hlDelta1m', \n",
    "        'cumChg1m', 'cumChg3m', 'cumChg6m']\n",
    "secpx[show].hist();\n",
    "secpx[show].tail(252*4).plot(secondary_y='pctChgStds');\n",
    "show = ['perc52WkLow']\n",
    "secpx[show].tail(252*4).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show = ['pctChgStds', 'mktHistPctChgStds']\n",
    "secpx[show].hist();\n",
    "secpx[show].tail(252*4).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# rolling SPY correlations\n",
    "secpx['crossCorr1m'] = closepx.pct_change().rolling(20).corr(SPY_pct_chg)\n",
    "secpx['crossCorr3m'] = closepx.pct_change().rolling(60).corr(SPY_pct_chg)\n",
    "\n",
    "show = [bench_symbol, 'crossCorr1m', 'crossCorr3m']\n",
    "secpx[show].tail(252*4).plot(secondary_y=bench_symbol);\n",
    "# closepx.pct_change().corr(SPY_pct_chg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# px_close.loc[pred_X.index, keep_bench].tail(20)\n",
    "# pred_X.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pred_X.fillna(method='ffill', inplace=True)\n",
    "# na = pred_X.tail(120).isna().any(0)\n",
    "# pred_X[na[na.values].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Fixing timeseries concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "super_list = []\n",
    "for n, t in enumerate(keep_sect):\n",
    "    try:\n",
    "        df = get_symbol_pricing(t, freq, cols=None)\n",
    "        rename_col(df, 'close', t)\n",
    "        print(\"Retrieving pricing: {0}, {1}\".format(t, df.shape))\n",
    "        df.index = df.index.strftime('%Y-%m-%d')\n",
    "        super_list.append(df[t])\n",
    "    except Exception as e:\n",
    "        print(\"Exception, get_mults_pricing: {0}\\n{1}\".format(t, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame(super_list[0])\n",
    "if len(super_list[1:]):\n",
    "    for x in super_list[1:]: full_df = pd.merge(full_df, x, left_index=True, right_index=True, how='outer')\n",
    "full_df.index = pd.to_datetime(full_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "super_list[0].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "super_list[1].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "super_list[2].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_symbol_pricing('JPY=X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_df.tail(30).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for d in super_list:\n",
    "    print(d.shape, d.drop_duplicates().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "super_list[1].duplicated().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat(super_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "super_list[0].tail(10).index, \\\n",
    "super_list[1].tail(10).index, \\\n",
    "super_list[2].tail(10).index, \\\n",
    "super_list[3].tail(10).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ss = get_symbol_pricing('DX-Y.NYB', freq).tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ss.index.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pd.merge(super_list[1], super_list[2], left_index=True, right_index=True)\n",
    "# pd.concat(super_list, axis=1)\n",
    "init_df = pd.DataFrame(super_list[0])\n",
    "for x in super_list[1:]:\n",
    "    init_df = pd.merge(init_df, x, left_index=True, right_index=True)\n",
    "init_df.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.concat(super_list, axis=1, ).tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for d in super_list: d.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat(super_list, axis=1, copy=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
